# ==============================================================================
# Ollama (本地模型) Provider Configuration
# ==============================================================================
# OpenClaw 模型配置文件
# 文档: https://docs.openclaw.ai/providers/ollama
# ==============================================================================

# 模型提供商类型
provider: ollama

# 显示名称
label: "Ollama (本地 Llama)"

# API 端点 (本地)
apiBaseUrl: http://localhost:11434

# 使用的模型
model: llama3.1:8b

# 可用模型列表 (用于回退)
models:
  - llama3.1:8b      # 推荐 (英文简历)
  - llama3.1:70b     # 高质量 (需要大内存)
  - mistral:7b       # 轻量快速
  - qwen2.5:7b       # 中文支持好

# 最大 token 数量
maxTokens: 8192

# 请求超时 (毫秒)
timeout: 300000

# 最大重试次数
maxRetries: 2

# 温度参数
temperature: 0.7

# 系统提示词
systemPrompt: |
  You are a professional resume writer specialized in embedded software engineering.
  Help create professional, ATS-friendly resumes.

# 是否启用
enabled: false

# 是否为默认模型
default: false

# 本地模型不需要代理
# 重要: 完全离线运行，保护隐私

# 高级设置
advanced:
  # 强制使用 CPU (如果有 GPU 但不想用)
  # forceCpu: false

  # GPU 层数 (默认全部)
  # gpuLayers: 0

  # 上下文大小
  # contextSize: 8192

  # 线程数
  # numThread: 4

  # 保持模型加载 (默认 5 分钟)
  # keepAlive: 5m
